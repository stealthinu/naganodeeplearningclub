{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Keras Fashion MNIST",
   "version": "0.3.2",
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb",
     "timestamp": 1538808473360
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "metadata": {
    "id": "Ot79jiI7GiHR",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "# Fashion MNIST with Keras and TPUs\n",
    "\n",
    "Let's try out using `tf.keras` and Cloud TPUs to train a model on the fashion MNIST dataset.\n",
    "\n",
    "First, let's grab our dataset using `tf.keras.datasets`."
   ]
  },
  {
   "metadata": {
    "id": "Zo-Yk6LFGfSf",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "486584a5-9c77-469c-8cca-ed9bb6246e44",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1539256336649,
     "user_tz": -540,
     "elapsed": 2879,
     "user": {
      "displayName": "Kiyoshi SATOH",
      "photoUrl": "",
      "userId": "17263090257850830810"
     }
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# add empty color dimension\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "26435584/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "id": "Hgc2FZKVMx15",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "# Defining our model\n",
    "\n",
    "We will use a standard conv-net for this example.  We have 3 layers with drop-out and batch normalization between each layer."
   ]
  },
  {
   "metadata": {
    "id": "W7gMbs70GxA7",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "outputId": "d503bdc5-fcf6-41aa-b85c-cc9fc0b1bf67",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1539256610518,
     "user_tz": -540,
     "elapsed": 1331,
     "user": {
      "displayName": "Kiyoshi SATOH",
      "photoUrl": "",
      "userId": "17263090257850830810"
     }
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.summary()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 28, 28, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 256)       6656      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 3,878,670\n",
      "Trainable params: 3,877,644\n",
      "Non-trainable params: 1,026\n",
      "_________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "id": "xLeZATVaNAnE",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "# Training on the TPU\n",
    "\n",
    "We're ready to train!   We first construct our model on the TPU, and compile it.\n",
    "\n",
    "Here we demonstrate that we can use a generator function and `fit_generator` to train the model.  You can also pass in `x_train` and `y_train` to `tpu_model.fit()` instead."
   ]
  },
  {
   "metadata": {
    "id": "pWEYmd_hIWg8",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "outputId": "04f7d057-c602-4589-890c-cac30dff3578",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1539256745950,
     "user_tz": -540,
     "elapsed": 130265,
     "user": {
      "displayName": "Kiyoshi SATOH",
      "photoUrl": "",
      "userId": "17263090257850830810"
     }
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    model,\n",
    "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "    )\n",
    ")\n",
    "tpu_model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=1e-3, ),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "def train_gen(batch_size):\n",
    "  while True:\n",
    "    offset = np.random.randint(0, x_train.shape[0] - batch_size)\n",
    "    yield x_train[offset:offset+batch_size], y_train[offset:offset + batch_size]\n",
    "    \n",
    "\n",
    "tpu_model.fit_generator(\n",
    "    train_gen(1024),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.60.233.10:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14142305961289128334)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2579023833871048289)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 15915402538951661186)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2267227983516991828)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7100023764453572343)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12652469193966477752)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10090562730875573346)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9502912903535046055)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11695754909662808496)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 1251419194191845433)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17298465049082444398)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 8537206450371460455)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 28, 28, 1), dtype=tf.float32, name=u'batch_normalization_3_input0'), TensorSpec(shape=(128, 1), dtype=tf.float32, name=u'activation_3_target0')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for batch_normalization_3_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 3.47807908058 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.5394 - sparse_categorical_accuracy: 0.6948INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 28, 28, 1), dtype=tf.float32, name=u'batch_normalization_3_input0'), TensorSpec(shape=(128, 1), dtype=tf.float32, name=u'activation_3_target0')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for batch_normalization_3_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 2.24770307541 secs\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(98, 28, 28, 1), dtype=tf.float32, name=u'batch_normalization_3_input0'), TensorSpec(shape=(98, 1), dtype=tf.float32, name=u'activation_3_target0')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for batch_normalization_3_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 1.95504713058 secs\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 1.5299 - sparse_categorical_accuracy: 0.6955 - val_loss: 2.0478 - val_sparse_categorical_accuracy: 0.4128\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.5117 - sparse_categorical_accuracy: 0.8320 - val_loss: 1.3711 - val_sparse_categorical_accuracy: 0.5600\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.3938 - sparse_categorical_accuracy: 0.8653 - val_loss: 0.9118 - val_sparse_categorical_accuracy: 0.6768\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.3324 - sparse_categorical_accuracy: 0.8837 - val_loss: 0.4214 - val_sparse_categorical_accuracy: 0.8424\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.2776 - sparse_categorical_accuracy: 0.8994 - val_loss: 0.3121 - val_sparse_categorical_accuracy: 0.8864\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.2515 - sparse_categorical_accuracy: 0.9078 - val_loss: 0.2756 - val_sparse_categorical_accuracy: 0.9032\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.2083 - sparse_categorical_accuracy: 0.9243 - val_loss: 0.2469 - val_sparse_categorical_accuracy: 0.9144\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.1915 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.2501 - val_sparse_categorical_accuracy: 0.9072\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.1679 - sparse_categorical_accuracy: 0.9382 - val_loss: 0.2318 - val_sparse_categorical_accuracy: 0.9184\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.1613 - sparse_categorical_accuracy: 0.9395 - val_loss: 0.2537 - val_sparse_categorical_accuracy: 0.9152\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f515cbac550>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 5
    }
   ]
  },
  {
   "metadata": {
    "id": "ESL6ltQTMm05",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "# Checking our results (inference)\n",
    "\n",
    "Now that we're done training, let's see how well we can predict fashion categories!  Keras/TPU prediction isn't working due to a small bug (fixed in TF 1.12!), but we can predict on the CPU to see how our results look."
   ]
  },
  {
   "metadata": {
    "id": "SaYPv_aKId2d",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
    "\n",
    "\n",
    "cpu_model = tpu_model.sync_to_cpu()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(images, predictions):\n",
    "  n = images.shape[0]\n",
    "  nc = int(np.ceil(n / 4))\n",
    "  f, axes = pyplot.subplots(nc, 4)\n",
    "  for i in range(nc * 4):\n",
    "    y = i // 4\n",
    "    x = i % 4\n",
    "    axes[x, y].axis('off')\n",
    "    \n",
    "    label = LABEL_NAMES[np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i])\n",
    "    if i > n:\n",
    "      continue\n",
    "    axes[x, y].imshow(images[i])\n",
    "    axes[x, y].text(0.5, 0.5, label + '\\n%.3f' % confidence, fontsize=14)\n",
    "\n",
    "  pyplot.gcf().set_size_inches(8, 8)  \n",
    "\n",
    "plot_predictions(np.squeeze(x_test[:16]), \n",
    "                 cpu_model.predict(x_test[:16]))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "GJAaFlQYNhoW",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "# Not bad!"
   ]
  }
 ]
}
