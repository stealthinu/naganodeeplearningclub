{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "cifar10_10min",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {
    "id": "MYEk56H7Hh-a",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "# CIFAR10を10分で学習\n",
    "\n",
    "CIFAR10をColabのGPU環境で10分だけ学習し、テストデータで一番認識率が高くなるものを目指します。"
   ]
  },
  {
   "metadata": {
    "id": "Dd42HoIOBEBX",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## 初期設定"
   ]
  },
  {
   "metadata": {
    "id": "PZrd9lCH4coU",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Flatten, Dropout, Add\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.legend(['acc', 'val_acc'], loc='lower right')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.legend(['loss', 'val_loss'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def test_cifar10():\n",
    "    test_num = 10\n",
    "    start = np.random.randint(X_test.shape[0] - test_num)\n",
    "    x_test = X_test[start:start+test_num]\n",
    "    y_test = Y_test[start:start+test_num]\n",
    "\n",
    "    fig, subplts = plt.subplots(ncols=10, figsize=(20, 2))\n",
    "    for i in range(10):\n",
    "        image = x_test[i]\n",
    "        two_d = (np.reshape(image, (32, 32, 3)) * 255).astype(np.uint8)\n",
    "        subplts[i].axis('off')\n",
    "        subplts[i].imshow(two_d, interpolation='nearest')\n",
    "\n",
    "    print(np.argmax(y_test, axis=1))\n",
    "\n",
    "    preds = model.predict(x_test)\n",
    "    print(np.argmax(preds, axis=1))\n",
    "    \n",
    "# CIFAR10データ読み込み\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "Y_train = to_categorical(y_train, 10)\n",
    "Y_test = to_categorical(y_test, 10)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "k9F_W_Ne-1qh",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## ベースライン(LeNet)\n",
    "\n",
    "LeNet のCNN  \n",
    "LeNetは1998にLeCun先生が作られたCNNの直接の先祖となったネットワーク"
   ]
  },
  {
   "metadata": {
    "id": "5w61cYN99TJE",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "eaZA5YSZ5CrS",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(momentum=0.9, nesterov=True),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "qzpUDxLG5Ge8",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "8WkZro2B5NK2",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "HhnwNKVl5Taj",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "test_cifar10()\n",
    "# 0:airplane, 1:automobile, 2:bird, 3:cat, 4:deer, 5:dog, 6:frog, 7:horse, 8:ship, 9:truck"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "C6_56Km0IYnx",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet結果\n",
    "4.1s/epoch  \n",
    "150epoch/10min  \n",
    "68%"
   ]
  },
  {
   "metadata": {
    "id": "jh3PMZ7kCX97",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## オプティマイザー変更\n",
    "\n",
    "重みの変更手法をSGDより高速に学習するものを利用します。  "
   ]
  },
  {
   "metadata": {
    "id": "7SS3CSUbLWhU",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "### Adam\n",
    "\n",
    "よく使われているAdamを利用してみます。  "
   ]
  },
  {
   "metadata": {
    "id": "e6nW19Qbr6dK",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "wmBpmBGfDiAE",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "6i3tyTQgEqvt",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "UmMv0_wAEvzm",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "15Xq7nllLo4b",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-Adam結果\n",
    "4.1s/epoch  \n",
    "30epoch程で上限  \n",
    "70%"
   ]
  },
  {
   "metadata": {
    "id": "okCIyoRPGapk",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "### Adabound\n",
    "\n",
    "Adam は学習速度が高いが汎化能力がSGDよりも悪くなると言われています。  \n",
    "Adaboundという改良手法が最近出て注目されているので試してみましょう。\n",
    "\n",
    "https://qiita.com/Phoeboooo/items/f610affdcaaae0a28f34  \n",
    "https://github.com/CyberZHG/keras-adabound"
   ]
  },
  {
   "metadata": {
    "id": "CnWWx5R-OqJM",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "class AdaBound(keras.optimizers.Optimizer):\n",
    "    \"\"\"AdamBound optimizer.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        final_lr: float >= 0. Final (SGD) learning rate.\n",
    "        base_lr: float >= 0. Used for loading the optimizer. Do not set the argument manually.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        gamma: float, 0 < gamma < 1. Convergence speed of the bound functions.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this algorithm.\n",
    "    # References\n",
    "        - [Adaptive Gradient Methods with Dynamic Bound of Learning Rate]\n",
    "          (https://openreview.net/forum?id=Bkg3g2R9FX)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, final_lr=0.1, base_lr=None,\n",
    "                 beta_1=0.9, beta_2=0.999, gamma=0.001,\n",
    "                 epsilon=None, decay=0., amsgrad=False, **kwargs):\n",
    "        super(AdaBound, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.final_lr = K.variable(final_lr, name='final_lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.gamma = K.variable(gamma, name='gamma')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        if base_lr is None:\n",
    "            self.base_lr = lr\n",
    "        else:\n",
    "            self.base_lr = base_lr\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "        final_lr = self.final_lr * lr / self.base_lr\n",
    "        lower_bound = final_lr * (1.0 - 1.0 / (self.gamma * t + 1.0))\n",
    "        upper_bound = final_lr * (1.0 + 1.0 / (self.gamma * t))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                step = lr_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                step = lr_t / (K.sqrt(v_t) + self.epsilon)\n",
    "            p_t = p - K.minimum(K.maximum(step, lower_bound), upper_bound) * m_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'final_lr': float(K.get_value(self.final_lr)),\n",
    "                  'base_lr': self.base_lr,\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'gamma': float(K.get_value(self.gamma)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(AdaBound, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "      "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "OrGB91ySuMWJ",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "QcLKKaQqGXc5",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=AdaBound(lr=1e-3, final_lr=0.1),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "AfL-fZu_PFe9",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "V6F4XXydnPft",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "kKfD75lHLtGx",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-Adabound結果\n",
    "4.1s/epoch  \n",
    "30epoch程で上限  \n",
    "70%"
   ]
  },
  {
   "metadata": {
    "id": "I13rkK76C5d2",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Batch Normalization の利用\n",
    "\n",
    "Batch Normalizationは学習時のバックプロパゲーションの値を、バッチ毎によい分布になるように変換します。  \n",
    "Batch Normalizationを利用することで高速に学習が進み認識率も高まります。"
   ]
  },
  {
   "metadata": {
    "id": "V84RLDzU5gDZ",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "ElGe6EzGElAd",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "OpZbz74gEl1-",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "-KuscUZ9Eu8H",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "wgS1pNiHRR1Y",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-BN-Adam結果\n",
    "5.1s/epoch\n",
    "30epoch程で上限  \n",
    "71%\n",
    "\n",
    "学習データでは10エポック強でほぼ100%になっていることがわかります。"
   ]
  },
  {
   "metadata": {
    "id": "J5Ud5fziGvbn",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## 層を広くする\n",
    "\n",
    "特徴を分類するニューロンを増やして、層を広く(wide)することで性能が上がることがあります。  \n",
    "ここでは中間層のニューロンをすべて4倍にしてみます。"
   ]
  },
  {
   "metadata": {
    "id": "ODUT73jSI94d",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(80, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(200, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2000))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "iJGnSMHZ6qea",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "s14_10BG7olf",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "7arjDl4TnSfF",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "hdPJ55Vp84tg",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-4wide-Adam結果\n",
    "17.3s/epoch  \n",
    "30epoch程で上限  \n",
    "73%"
   ]
  },
  {
   "metadata": {
    "id": "vy6kArvdFZVj",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## 層を深くする\n",
    "\n",
    "層を深くすることで表現力を上げることができます。  \n",
    "基本的にディープラーニングでは深い大きなネットワークにすることで性能が上がってきました。  \n",
    "しかし単に層を深くしただけではそれほど性能が上がらないこともわかります。"
   ]
  },
  {
   "metadata": {
    "id": "nxgUFgJHFh3h",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(20, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "dLg5gl2viwZ1",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "GNU_kcPgixEs",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=20)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "rhjCzw3ZnTz3",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "mQx1PT18i3Nr",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-2deep-Adam結果\n",
    "12.2s/epoch  \n",
    "50epoch/10min  \n",
    "73%"
   ]
  },
  {
   "metadata": {
    "id": "jy_dJY0e6vqH",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Conv2Dを軽量化する\n",
    "\n",
    "5x5 は 3x3 + 3x3 で範囲が同じになり計算量を減らすことができます。  \n",
    "さらに 3x3 は 3x1 + 1x3 で同じ範囲になると考えることができます。  \n",
    "\n",
    "https://www.slideshare.net/ren4yu/deep-neural-network-79382352"
   ]
  },
  {
   "metadata": {
    "id": "dG0kuBkR6u1m",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(20, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(50, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "JidcS69Xnn_b",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "BGi4cR7vnuiJ",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=25)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "ViW--Yu9nx-C",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "mria-liZny32",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-3x3+3x3-Adam結果\n",
    "6.1s/epoch  \n",
    "30epoch  \n",
    "71%"
   ]
  },
  {
   "metadata": {
    "id": "ANsmdOEXkxq0",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (1, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(20, (3, 1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(20, (1, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(20, (3, 1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (1, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(50, (3, 1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(50, (1, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(50, (3, 1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "colab_type": "code",
    "id": "JjveTRJLpJ8v",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "colab_type": "code",
    "id": "pFFu1eGZpJ8z",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "colab_type": "code",
    "id": "bhBEhQxApJ84",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "colab_type": "text",
    "id": "op_iveDMpU6s"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-3x1+1x3-Adam結果\n",
    "9.2s/epoch  \n",
    "30epoch  \n",
    "69%"
   ]
  },
  {
   "metadata": {
    "id": "FX_A-gjuIu5y",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## ResNet\n",
    "\n",
    "残差ブロックと言う畳み込みなどの層をスキップした結合を持つことで、非常に深いネットワークでも学習が進むようにしたものです。\n",
    "\n",
    "https://deepage.net/deep_learning/2016/11/30/resnet.html\n",
    "\n",
    "Kerasでの実装はFunctional APIを使う必要があるため、他のサンプルとは違った書き方になります。  \n",
    "ここでは@koshian2氏の実装を使わせていただいています。\n",
    "\n",
    "https://qiita.com/koshian2/items/343a55d59d8fdc112661"
   ]
  },
  {
   "metadata": {
    "id": "p_L9NpyIJHqb",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "nb_blocks = 9\n",
    "input = Input(shape=(32, 32, 3))\n",
    "X = input\n",
    "n_filter = 16\n",
    "for i in range(nb_blocks):\n",
    "    # 3ブロック単位でAveragePoolingを入れる、フィルター数を倍にする\n",
    "    if i % 3 == 0 and i != 0:\n",
    "        X = AveragePooling2D((2,2))(X)\n",
    "        n_filter *= 2\n",
    "    # ショートカットとメインのフィルター数を揃えるために活性化関数なしの畳込みレイヤーを作る\n",
    "    if i % 3 == 0:\n",
    "        X = Conv2D(n_filter, (3,3), padding=\"same\")(X)\n",
    "    # 1ブロック単位の処理\n",
    "    # ショートカット：ショートカット→BatchNorm（ResBlockを使う場合のみ）\n",
    "    shortcut = X\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    # メイン\n",
    "    # 畳み込み→BatchNorm→活性化関数\n",
    "    X = Conv2D(n_filter, (3,3), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    # 畳み込み→BatchNorm\n",
    "    X = Conv2D(n_filter, (3,3), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    # ショートカットとマージ（ResBlockを使う場合のみ）\n",
    "    X = Add()([X, shortcut])\n",
    "    # 活性化関数\n",
    "    X = Activation(\"relu\")(X)\n",
    "# 全結合\n",
    "X = Flatten()(X)\n",
    "y = Dense(10, activation=\"softmax\")(X)\n",
    "# モデル\n",
    "model = Model(inputs=input, outputs=y)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "7z_OzGyK9gvX",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "VVOu00qJ-VP3",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=10)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "EKrmYYue_6uB",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "Z7VbaX_p-ZXT",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### ResNet-9blocks 結果\n",
    "28.5s/epoch  \n",
    "20epoch/10min    \n",
    "80%"
   ]
  },
  {
   "metadata": {
    "id": "7XIK9ivL-iQ5",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## バッチサイズを小さくする\n",
    "\n",
    "バッチサイズが大きいほうが計算速度が速くなります。  \n",
    "しかしバッチサイズが大きすぎると汎化能力が落ちると言われています。  \n",
    "\n",
    "https://tech.nikkeibp.co.jp/dm/atcl/mag/15/00144/00002/"
   ]
  },
  {
   "metadata": {
    "id": "6nhmTGQMrfJi",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "71IOYaLu-24K",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "Hw_3PGzCruhY",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=128, epochs=5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "NHMdOKjdsZBt",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "KaBcEhodseOl",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-128batch-Adam結果\n",
    "8.2s/epoch  \n",
    "20epoch程で上限  \n",
    "70%"
   ]
  },
  {
   "metadata": {
    "id": "NK_c91ahJh50",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Dropout の利用\n",
    "\n",
    "確率的にニューロンを動かなくさせる（なかったことにする）ことで汎化能力があがる手法です。  \n",
    "そのかわり学習速度は遅くなります。  \n",
    "Batch Normalizationにより必要性はだいぶなくなりましたが、汎化能力を増すために今も利用されていると思います。"
   ]
  },
  {
   "metadata": {
    "id": "Xf3ab6xRJmZ5",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "N2TrfYJB_3EB",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "Qld-EZ0H__bA",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=100)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "DTiupkJR_8YJ",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "oxXsMPPHADtD",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-Dropout-Adam結果\n",
    "5.1s/epoch  \n",
    "110epoch/10min  \n",
    "76%"
   ]
  },
  {
   "metadata": {
    "id": "MT7bEbYzFVh2",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## データ拡張\n",
    "\n",
    "データを少しずらしたり回転したり拡縮したりなどで少しづつ違うデータで学習することで汎化能力を上げることができます。  \n",
    "Kerasでは画像のデータ拡張のための関数が準備されています。\n",
    "\n",
    "http://aidiary.hatenablog.com/entry/20161212/1481549365"
   ]
  },
  {
   "metadata": {
    "colab_type": "code",
    "id": "98GaA8wnE1oo",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "colab_type": "code",
    "id": "JwLWaL_FE1os",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "0272eVOtFX1y",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.05,  # set range for random shear\n",
    "    zoom_range=0.1,  # set range for random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "AmufNItitZp7",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                                 batch_size=1024),\n",
    "                    epochs=25,\n",
    "                    validation_data=(X_test, Y_test))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "TBBLO536HhFT",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "plot_history(history)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "svanGiIuygJw",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### LeNet-DataAugmentation-Adam結果\n",
    "23.5s/epoch  \n",
    "25epoch/10min  \n",
    "72%"
   ]
  },
  {
   "metadata": {
    "id": "UmmTinYlJs0p",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Mixup / BC-Learning\n",
    "\n",
    "複数のデータを透過合成してデータ拡張をする手法です。\n",
    "\n",
    "https://qiita.com/yu4u/items/70aa007346ec73b7ff05\n",
    "\n",
    "https://qiita.com/koshian2/items/909360f50e3dd5922f32\n",
    "\n",
    "ここでは@koshian2氏のBC-Learningの実装を使わせていただいています。\n",
    "\n",
    "https://qiita.com/koshian2/items/d0661842eb66a7c0c0f3\n"
   ]
  },
  {
   "metadata": {
    "id": "G6f61-7YRhB_",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import History, LearningRateScheduler\n",
    "from tensorflow.contrib.tpu.python.tpu import keras_support\n",
    "\n",
    "import os, json\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def conv_bn_relu(input, ch):\n",
    "    x = layers.Conv2D(ch, 3, padding=\"same\")(input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "def create_network():\n",
    "    input = layers.Input((32,32,3))\n",
    "    x = input\n",
    "    for i in range(3):\n",
    "        x = conv_bn_relu(x, 64)\n",
    "    x = layers.AveragePooling2D(2)(x)\n",
    "    for i in range(3):\n",
    "        x = conv_bn_relu(x, 128)\n",
    "    x = layers.AveragePooling2D(2)(x)\n",
    "    for i in range(3):\n",
    "        x = conv_bn_relu(x, 256)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(input, x)\n",
    "\n",
    "def normal_generator(X, y, batch_size):\n",
    "    while True:\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        for i in range(X.shape[0]//batch_size):\n",
    "            current_indices = indices[i*batch_size:(i+1)*batch_size]\n",
    "            X_batch = (X[current_indices] / 255.0).astype(np.float32)\n",
    "            y_batch = y[current_indices]\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    true_label = K.argmax(y_true, axis=-1)\n",
    "    pred_label = K.argmax(y_pred, axis=-1)\n",
    "    return K.cast(K.equal(true_label, pred_label), \"float\")\n",
    "\n",
    "def bclearning_generator(base_generator, batch_size, sample_steps, n_steps):\n",
    "    assert batch_size >= sample_steps\n",
    "    assert batch_size % sample_steps == 0\n",
    "    X_cache, y_cache = [], []\n",
    "    while True:\n",
    "        for i in range(n_steps):\n",
    "            while True:\n",
    "                current_images, current_onehots = next(base_generator)\n",
    "                if current_images.shape[0] == sample_steps and current_onehots.shape[0] == sample_steps:\n",
    "                    break\n",
    "            current_labels = np.sum(np.arange(current_onehots.shape[1]) * current_onehots, axis=-1)\n",
    "            for j in range(batch_size//sample_steps):\n",
    "                for k in range(sample_steps):\n",
    "                    diff_indices = np.where(current_labels != current_labels[k])[0]\n",
    "                    mix_ind = np.random.choice(diff_indices)\n",
    "                    rnd = np.random.rand()\n",
    "                    if rnd < 0.5: rnd = 1.0 - rnd # 主画像を偏らさないために必要\n",
    "                    mix_img = rnd * current_images[k] + (1.0-rnd) * current_images[mix_ind]\n",
    "                    mix_onehot = rnd * current_onehots[k] + (1.0-rnd) * current_onehots[mix_ind]\n",
    "                    X_cache.append(mix_img)\n",
    "                    y_cache.append(mix_onehot)\n",
    "            X_batch = np.asarray(X_cache, dtype=np.float32) / 255.0\n",
    "            y_batch = np.asarray(y_cache, dtype=np.float32)\n",
    "            X_cache, y_cache = [], []\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "def step_decay(epoch):\n",
    "    x = 1e-3\n",
    "    if epoch >= 100: return 2e-4\n",
    "    elif epoch >= 150: return 4e-5\n",
    "    elif epoch >= 200: return 8e-6\n",
    "    return x\n",
    "\n",
    "def train(use_bc, step_size):\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    model = create_network()\n",
    "    if use_bc:\n",
    "        model.compile(\"adam\", \"kullback_leibler_divergence\", [acc])\n",
    "    else:\n",
    "        model.compile(tf.train.AdamOptimizer(1e-3), \"categorical_crossentropy\", [\"acc\"])\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    if use_bc:\n",
    "        base_gen = ImageDataGenerator(horizontal_flip=True, width_shift_range=4.0/32.0, \n",
    "                                      height_shift_range=4.0/32.0).flow(X_train, y_train, step_size)\n",
    "        train_gen = bclearning_generator(base_gen, batch_size, step_size, X_train.shape[0]//step_size)\n",
    "    else:\n",
    "        train_gen = normal_generator(X_train, y_train, batch_size)\n",
    "    val_gen = normal_generator(X_test, y_test, step_size)\n",
    "\n",
    "#    tpu_grpc_url = \"grpc://\"+os.environ[\"COLAB_TPU_ADDR\"]\n",
    "#    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
    "#    strategy = keras_support.TPUDistributionStrategy(tpu_cluster_resolver)\n",
    "#    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
    "\n",
    "    hist = History()\n",
    "    scheduler = LearningRateScheduler(step_decay)\n",
    "\n",
    "    model.fit_generator(train_gen, steps_per_epoch=X_train.shape[0]//step_size,\n",
    "                        validation_data=val_gen, validation_steps=X_test.shape[0]//step_size,\n",
    "                        callbacks=[hist, scheduler], epochs=10)\n",
    "#                        callbacks=[hist, scheduler], epochs=250)\n",
    "\n",
    "    history = hist.history\n",
    "    with open(f\"bc_learning_{use_bc}_{step_size}.json\", \"w\") as fp:\n",
    "        json.dump(history, fp)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    K.clear_session()\n",
    "    train(True, 128)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "5mYzo8F9WtOM",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### Mixup / BC-Learning結果\n",
    "\n",
    "53s/epoch  \n",
    "10epoch/min  \n",
    "82%"
   ]
  },
  {
   "metadata": {
    "id": "y_rgLyjf5d-y",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## KerasのFashion-MNIST用のsample\n",
    "\n",
    "KerasからTPUを使うためのサンプルでFasion-MNISTを学習するためのサンプル用ネットワークがありますので参考に。  \n",
    "https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb"
   ]
  },
  {
   "metadata": {
    "id": "4V8EQ1Hy5uFM",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "ot5i3AnyCfR5",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=1024, epochs=5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "CcOHupEPEMah",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "#### Keras Fashion-MNIST sample結果\n",
    "40s/epoch  \n",
    "15epoch/min  \n",
    "80%"
   ]
  },
  {
   "metadata": {
    "id": "k5H7Rr3bOedb",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "K.clear_session()"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}
